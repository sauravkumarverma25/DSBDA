{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01094014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saurav kumar verma\n",
    "# Roll no:TECO2223B063\n",
    "# Batch-B3 \n",
    "# TextAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2454756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3929f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bc3e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample document\n",
    "doc = \"Text analytics is the process of deriving insights from text data. It involves natural language processing techniques such as tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization. The goal of text analytics is to transform unstructured text data into structured data that can be analyzed to gain insights and make informed decisions.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a0770e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the document\n",
    "tokens = word_tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8c59c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba8daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9448ffc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d40961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform POS tagging\n",
    "pos_tokens = pos_tag(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f960df3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: Text analytics is the process of deriving insights from text data. It involves natural language processing techniques such as tokenization, part-of-speech tagging, stop words removal, stemming, and lemmatization. The goal of text analytics is to transform unstructured text data into structured data that can be analyzed to gain insights and make informed decisions.\n",
      "Tokenized Text: ['Text', 'analytics', 'is', 'the', 'process', 'of', 'deriving', 'insights', 'from', 'text', 'data', '.', 'It', 'involves', 'natural', 'language', 'processing', 'techniques', 'such', 'as', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'and', 'lemmatization', '.', 'The', 'goal', 'of', 'text', 'analytics', 'is', 'to', 'transform', 'unstructured', 'text', 'data', 'into', 'structured', 'data', 'that', 'can', 'be', 'analyzed', 'to', 'gain', 'insights', 'and', 'make', 'informed', 'decisions', '.']\n",
      "Filtered Text (Stop Words Removed): ['Text', 'analytics', 'process', 'deriving', 'insights', 'text', 'data', '.', 'involves', 'natural', 'language', 'processing', 'techniques', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'words', 'removal', ',', 'stemming', ',', 'lemmatization', '.', 'goal', 'text', 'analytics', 'transform', 'unstructured', 'text', 'data', 'structured', 'data', 'analyzed', 'gain', 'insights', 'make', 'informed', 'decisions', '.']\n",
      "Stemmed Text: ['text', 'analyt', 'process', 'deriv', 'insight', 'text', 'data', '.', 'involv', 'natur', 'languag', 'process', 'techniqu', 'token', ',', 'part-of-speech', 'tag', ',', 'stop', 'word', 'remov', ',', 'stem', ',', 'lemmat', '.', 'goal', 'text', 'analyt', 'transform', 'unstructur', 'text', 'data', 'structur', 'data', 'analyz', 'gain', 'insight', 'make', 'inform', 'decis', '.']\n",
      "Lemmatized Text: ['Text', 'analytics', 'process', 'deriving', 'insight', 'text', 'data', '.', 'involves', 'natural', 'language', 'processing', 'technique', 'tokenization', ',', 'part-of-speech', 'tagging', ',', 'stop', 'word', 'removal', ',', 'stemming', ',', 'lemmatization', '.', 'goal', 'text', 'analytics', 'transform', 'unstructured', 'text', 'data', 'structured', 'data', 'analyzed', 'gain', 'insight', 'make', 'informed', 'decision', '.']\n",
      "POS Tagged Text: [('Text', 'JJ'), ('analytics', 'NNS'), ('process', 'NN'), ('deriving', 'VBG'), ('insights', 'NNS'), ('text', 'NN'), ('data', 'NNS'), ('.', '.'), ('involves', 'VBZ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('techniques', 'NNS'), ('tokenization', 'NN'), (',', ','), ('part-of-speech', 'JJ'), ('tagging', 'NN'), (',', ','), ('stop', 'VB'), ('words', 'NNS'), ('removal', 'JJ'), (',', ','), ('stemming', 'VBG'), (',', ','), ('lemmatization', 'NN'), ('.', '.'), ('goal', 'NN'), ('text', 'NN'), ('analytics', 'NNS'), ('transform', 'NN'), ('unstructured', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('structured', 'VBD'), ('data', 'NNS'), ('analyzed', 'VBN'), ('gain', 'NN'), ('insights', 'NNS'), ('make', 'VBP'), ('informed', 'JJ'), ('decisions', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Text:\", doc)\n",
    "print(\"Tokenized Text:\", tokens)\n",
    "print(\"Filtered Text (Stop Words Removed):\", filtered_tokens)\n",
    "print(\"Stemmed Text:\", stemmed_tokens)\n",
    "print(\"Lemmatized Text:\", lemmatized_tokens)\n",
    "print(\"POS Tagged Text:\", pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65bd8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "194fc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign documents\n",
    "d0 = 'hi hello howdy'\n",
    "d1 = 'hi'\n",
    "d2 = 'k9'\n",
    "\n",
    "# merge documents into a single corpus\n",
    "string = [d0, d1, d2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d30563d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# get tf-df values\n",
    "result = tfidf.fit_transform(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4445d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "hello : 1.6931471805599454\n",
      "hi : 1.2876820724517808\n",
      "howdy : 1.6931471805599454\n",
      "k9 : 1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "for ele1, ele2 in zip(tfidf.get_feature_names_out(), tfidf.idf_):\n",
    "\tprint(ele1, ':', ele2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c9ec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'hi': 1, 'hello': 0, 'howdy': 2, 'k9': 3}\n",
      "\n",
      "tf-idf value:\n",
      "  (0, 2)\t0.6227660078332259\n",
      "  (0, 0)\t0.6227660078332259\n",
      "  (0, 1)\t0.4736296010332684\n",
      "  (1, 1)\t1.0\n",
      "  (2, 3)\t1.0\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.62276601 0.4736296  0.62276601 0.        ]\n",
      " [0.         1.         0.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "\n",
    "# display tf-idf values\n",
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "# in matrix form\n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a122f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
